{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8bwGgGTy6NAq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/isizs/thesis/main/Temporal_Dataset_1992-01-01_1992-12-31_1_7.jsonl\"\n",
        "!wget {url}"
      ],
      "metadata": {
        "id": "duOY-pdyW2KH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration, T5Tokenizer\n",
        "import os\n",
        "import json\n",
        "import torch"
      ],
      "metadata": {
        "id": "2qN4GFx67Vxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "dataset = load_jsonl('Temporal_Dataset_1992-01-01_1992-12-31_1_7.jsonl')"
      ],
      "metadata": {
        "id": "8DEC11-t0W0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flan T5 base"
      ],
      "metadata": {
        "id": "cgy49349GH7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_name = \"google/flan-t5-base\"\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "COPFYznv7hha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_t5(prompt, max_length=100):\n",
        "    inputs == t5_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    outputs = t5_model.generate(**inputs, max_lemgth=max_length)\n",
        "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "bL8RzhZZ_un3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT 2\n"
      ],
      "metadata": {
        "id": "AtVRW3HVGaq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_name = \"gpt2\"\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_name)\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_name)"
      ],
      "metadata": {
        "id": "jHyeRWdoH56N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_gpt2(prompt,max_length=100):\n",
        "\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = gpt2_model.generate(**inputs, max_length=max_length)\n",
        "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "lWc5GplhNftr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama 3.2 1B"
      ],
      "metadata": {
        "id": "qW4oMTZYQIPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "NjqpzPeyQzqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama = \"meta-llama/Llama-3.2-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama)\n",
        "model = AutoModelForCausalLM.from_pretrained(llama)"
      ],
      "metadata": {
        "id": "Hn-VNzM3QH6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_llama(primpt, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=max_length)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Gz-wmyxGQgLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments(dataset, sample_size=100):\n",
        "    results = []\n",
        "\n",
        "    for i, item in enumerate(dataset[:sample_size]):\n",
        "        context = item['Term']\n",
        "        question = item['Question']\n",
        "        ground_truth = item['Answer']\n",
        "\n",
        "        t5_response = generate_response_t5(question)\n",
        "        gpt2_response = generate_response_gpt2(question)\n",
        "        llama_response = generate_response_llama(question)\n",
        "\n",
        "        results.append({\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            't5_response': t5_response,\n",
        "            'gpt2_response': gpt2_response,\n",
        "            'llama_response': llama_response,\n",
        "            'facts': item['Fact'],\n",
        "            'type': item['Type']\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1TdDRkoiE4jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = run_experiments(dataset, sample_size=100)\n",
        "\n",
        "with open('experiment_results.json', 'w') as f:\n",
        "    json.dump(experiment_results, f, indent=2)"
      ],
      "metadata": {
        "id": "iS-a_kunFu8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}